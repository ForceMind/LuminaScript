from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from sqlalchemy.orm import selectinload
from typing import List, Dict, Any
from pydantic import BaseModel 
import json

from database import init_db, get_db
import models
import schemas
import auth
from services import llm  # Import LLM Service
import logging
import sys
from datetime import datetime
from dotenv import load_dotenv
from fastapi import Request
from fastapi.responses import StreamingResponse
import io
from urllib.parse import quote

try:
    from docx import Document as DocxDocument
except ImportError:
    DocxDocument = None

# Load environment variables from .env file if it exists
load_dotenv()
from database import init_db, get_db, SessionLocal
import database # needed for SessionLocal access in some scopes if not imported directly

# Configure Logging
logging.basicConfig(
    level=logging.INFO, # Changed to INFO to avoid too much noise but capture essential flows
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("lumina_backend")

# Initialize App
app = FastAPI(title="LuminaScript API", version="0.1.0")

if __name__ == "__main__":
    import uvicorn
    # Allow running this file directly for debugging
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True, log_level="info")

import asyncio

@app.on_event("startup")
async def on_startup():
    logger.info("æœåŠ¡å™¨æ­£åœ¨å¯åŠ¨...")
    await init_db()
    logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼ŒæœåŠ¡å‡†å¤‡å°±ç»ªã€‚")

@app.get("/")
async def root():
    logger.info("æ”¶åˆ°æ ¹è·¯å¾„è¯·æ±‚")
    return {"message": "æ¬¢è¿ä½¿ç”¨å¦™ç¬”æµå…‰ (LuminaScript) API"}

# --- Admin & Logging Helpers ---

async def check_admin(current_user: models.User = Depends(auth.get_current_user)):
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Admin privileges required")
    return current_user

async def log_login(user_id: int, ip: str, status: str):
    async with SessionLocal() as db:
        log = models.LoginLog(
             user_id=user_id, 
             ip_address=ip, 
             status=status, 
             timestamp=datetime.now().isoformat()
        )
        db.add(log)
        await db.commit()

async def log_ai_action(user_id: int, project_id: int, action: str, prompt: str, response: str, tokens: int):
    async with SessionLocal() as db:
        log = models.AIInteractionLog(
            user_id=user_id,
            project_id=project_id,
            action=action,
            prompt=prompt[:5000],  # Truncate if too long to save generic DB space
            response=response[:5000],
            tokens=tokens,
            timestamp=datetime.now().isoformat()
        )
        db.add(log)
        await db.commit()

# --- Admin Routes ---

@app.get("/admin/users", response_model=List[schemas.UserResponse])
async def admin_list_users(
    db: AsyncSession = Depends(get_db), 
    admin: models.User = Depends(check_admin)
):
    result = await db.execute(select(models.User))
    return result.scalars().all()

@app.get("/admin/logs/login", response_model=schemas.PaginatedLoginLogs)
async def admin_list_login_logs(
    page: int = 1,
    page_size: int = 20,
    db: AsyncSession = Depends(get_db),
    admin: models.User = Depends(check_admin)
):
    # Calculate offset
    offset = (page - 1) * page_size
    
    # 1. Get Total Count
    count_query = select(func.count()).select_from(models.LoginLog)
    total_result = await db.execute(count_query)
    total = total_result.scalar()

    # 2. Get Items
    result = await db.execute(
        select(models.LoginLog, models.User.username)
        .join(models.User, models.LoginLog.user_id == models.User.id)
        .order_by(models.LoginLog.timestamp.desc())
        .offset(offset)
        .limit(page_size)
    )
    
    logs = []
    for log, username in result:
        log_dict = log.__dict__
        log_dict['user_name'] = username
        logs.append(log_dict)
        
    return {"total": total, "items": logs}

@app.get("/admin/logs/ai", response_model=schemas.PaginatedAILogs)
async def admin_list_ai_logs(
    page: int = 1,
    page_size: int = 20,
    db: AsyncSession = Depends(get_db),
    admin: models.User = Depends(check_admin)
):
    offset = (page - 1) * page_size
    
    # 1. Get Total Count
    count_query = select(func.count()).select_from(models.AIInteractionLog)
    total_result = await db.execute(count_query)
    total = total_result.scalar()

    # 2. Get Items
    result = await db.execute(
        select(models.AIInteractionLog, models.User.username)
        .join(models.User, models.AIInteractionLog.user_id == models.User.id)
        .order_by(models.AIInteractionLog.timestamp.desc())
        .offset(offset)
        .limit(page_size)
    )
    logs = []
    for log, username in result:
        log_dict = log.__dict__
        log_dict['user_name'] = username
        logs.append(log_dict)
        
    return {"total": total, "items": logs}

# --- Auth Routes ---

@app.post("/token", response_model=schemas.Token)
async def login_for_access_token(
    request: Request,
    background_tasks: BackgroundTasks,
    form_data: OAuth2PasswordRequestForm = Depends(), 
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"æ”¶åˆ°ç™»å½•è¯·æ±‚: ç”¨æˆ·å={form_data.username}")
    # 1. Fetch user
    result = await db.execute(select(models.User).where(models.User.username == form_data.username))
    user = result.scalars().first()
    
    # è·å–çœŸå®IP (X-Forwarded-For ä¼˜å…ˆ)
    forwarded = request.headers.get("x-forwarded-for")
    if forwarded:
        ip = forwarded.split(",")[0].strip()
    else:
        ip = request.client.host
    
    # 2. Verify
    if not user:
        logger.warning(f"ç™»å½•å¤±è´¥: ç”¨æˆ· {form_data.username} ä¸å­˜åœ¨")
        # Log failed attempt (No user_id, use 0 or distinct log)
        # For simplicity, we skip logging unknown users or we need to change model to allow nullable user_id
    elif not auth.verify_password(form_data.password, user.hashed_password):
        logger.warning(f"ç™»å½•å¤±è´¥: ç”¨æˆ· {form_data.username} å¯†ç é”™è¯¯")
        background_tasks.add_task(log_login, user_id=user.id, ip=ip, status="failed")
        
    if not user or not auth.verify_password(form_data.password, user.hashed_password):
         raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    logger.info(f"ç”¨æˆ· {form_data.username} ç™»å½•æˆåŠŸ")
    background_tasks.add_task(log_login, user_id=user.id, ip=ip, status="success")
    
    # 3. Create Token
    access_token = auth.create_access_token(data={"sub": user.username})
    return {"access_token": access_token, "token_type": "bearer"}

@app.post("/auth/register", response_model=schemas.UserResponse)
async def register(user: schemas.UserCreate, db: AsyncSession = Depends(get_db)):
    # Check existing
    result = await db.execute(select(models.User).where(models.User.username == user.username))
    if result.scalars().first():
        raise HTTPException(status_code=400, detail="Username already registered")
    
    # Create
    hashed_pw = auth.get_password_hash(user.password)
    new_user = models.User(username=user.username, hashed_password=hashed_pw)
    db.add(new_user)
    await db.commit()
    await db.refresh(new_user)
    return new_user

@app.get("/users/me", response_model=schemas.UserResponse)
async def read_users_me(current_user: models.User = Depends(auth.get_current_user)):
    return current_user

# --- Project Management ---

@app.post("/projects/", response_model=schemas.ProjectResponse)
async def create_project(
    project: schemas.ProjectCreate, 
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    logger.info(f"ç”¨æˆ· {current_user.username} æ­£åœ¨åˆ›å»ºæ–°é¡¹ç›®ï¼ŒLogline: {project.logline[:50]}...")
    # 1. First step: Create the project record based on logline
    # Real implementation would call LLM here to analyze logline first, 
    # but for now we just save it.
    new_project = models.Project(
        title=project.title,
        logline=project.logline,
        project_type=project.project_type,
        owner_id=current_user.id
    )
    db.add(new_project)
    await db.commit()
    await db.refresh(new_project)
    
    logger.info(f"é¡¹ç›®åˆ›å»ºæˆåŠŸ ID: {new_project.id}")

    # Reload to ensure relationships (scenes) are loaded for Pydantic
    result = await db.execute(
        select(models.Project)
        .where(models.Project.id == new_project.id)
        .options(selectinload(models.Project.scenes))
    )
    return result.scalars().first()

@app.get("/projects/", response_model=List[schemas.ProjectResponse])
async def list_projects(
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    result = await db.execute(
        select(models.Project)
        .where(models.Project.owner_id == current_user.id)
        .options(selectinload(models.Project.scenes))
    )
    return result.scalars().all()


@app.delete("/projects/{project_id}")
async def delete_project(
    project_id: int, 
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    project = await db.get(models.Project, project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")
    
    # Mark as failed/deleted to stop background tasks
    project.status = models.ProcessingStatus.FAILED 
    await db.delete(project)
    await db.commit()
    return {"status": "success"}

@app.patch("/projects/{project_id}", response_model=schemas.ProjectResponse)
async def update_project(
    project_id: int,
    project_update: schemas.ProjectUpdate,
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    # Use select with options to eager load scenes to avoid MissingGreenlet error in response validation
    result = await db.execute(
        select(models.Project)
        .where(models.Project.id == project_id)
        .options(selectinload(models.Project.scenes))
    )
    project = result.scalars().first()

    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")
    
    if project_update.project_type:
        project.project_type = project_update.project_type
    
    await db.commit()
    await db.refresh(project)
    return project

class InteractionRequest(BaseModel):
    answer: str
    context_key: str

@app.post("/projects/{project_id}/interact")
async def submit_interaction(
    project_id: int,
    interaction: InteractionRequest,
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    logger.info(f"æ”¶åˆ°é¡¹ç›® {project_id} çš„äº¤äº’å›ç­”: Key={interaction.context_key}, Answer={interaction.answer}")
    
    result = await db.execute(
        select(models.Project).where(models.Project.id == project_id)
    )
    project = result.scalars().first()
    
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")

    # Update context
    # Note: sqlalchemy JSON field needs reassignment to trigger update
    current_context = dict(project.global_context) if project.global_context else {}
    
    # Special Handling: Reset
    if interaction.context_key == 'final_confirm' and interaction.answer == 'reset':
        logger.info(f"é¡¹ç›® {project_id} æ”¶åˆ°é‡ç½®è¯·æ±‚ï¼Œæ¸…ç©ºä¸Šä¸‹æ–‡é‡æ–°å¼€å§‹è®¾å®šæµç¨‹")
        project.global_context = {}
        project.next_step_cache = None
        project.project_type = "pending"
        await db.commit()
        return {"status": "reset", "context": {}}

    current_context[interaction.context_key] = interaction.answer
    project.global_context = current_context

    # Ensure project_type is synced if that was the key (legacy support)
    if interaction.context_key == 'project_type':
        project.project_type = interaction.answer
    
    # Handle Title Update specifically
    if interaction.context_key == 'title':
        logger.info(f"Checking title update. Proposed Title: '{interaction.answer}'")
        # Ensure we don't accidentally set the title to the question string if logic failed somewhere
        # Simple heuristic: If it ends with '?', it's likely a mistake.
        if interaction.answer and not interaction.answer.strip().endswith('?'):
            project.title = interaction.answer
            logger.info(f"Project Title Updated to: {project.title}")
        else:
             logger.warning(f"Ignored suspicious title update: {interaction.answer}")
        
    # Clear the cache because state has changed
    project.next_step_cache = None

    await db.commit()
    logger.info(f"é¡¹ç›® {project_id} ä¸Šä¸‹æ–‡å·²æ›´æ–°ï¼Œç¼“å­˜å·²æ¸…é™¤")
    return {"status": "updated", "context": project.global_context}


@app.post("/projects/{project_id}/analyze")
async def analyze_logline(
    project_id: int, 
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    """
    Phase 1: Deep Analysis & Setup.
    Iteratively helps the user build the 'Project Bible' by asking questions.
    """
    project = await db.get(models.Project, project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")

    # Check Cache First (For resuming sessions)
    if project.next_step_cache:
        logger.info(f"é¡¹ç›® {project_id} å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›ä¹‹å‰çš„æé—®ã€‚")
        return project.next_step_cache

    logger.info(f"æ­£åœ¨åˆ†æé¡¹ç›® {project_id} çš„è¿›åº¦çŠ¶å†µ...")

    context = project.global_context or {}
    
    # --- Definition of the 10-Step Setup Flow ---
    # Follow Snowflake Method concepts: Logline -> Expansion -> Characters -> Detailed Plot -> Confirmation
    REQUIRED_STEPS = [
        {"key": "project_type", "question": "æ‚¨æƒ³åˆ›ä½œå“ªç§ç±»å‹çš„å‰§æœ¬ï¼Ÿ", "default_options": [
             {"label": "ğŸ¥ ç”µå½±å‰§æœ¬ (Movie)", "value": "movie"},
             {"label": "ğŸ“º ç”µè§†å‰§ (TV Series)", "value": "tv"},
             {"label": "ğŸ“± ç°ä»£çŸ­å‰§ (Short Drama)", "value": "short"}
        ]},
        # Dynamic steps based on Project Type
        {"key": "movie_duration", "question": "ç”µå½±é¢„æœŸçš„æ—¶é•¿æ˜¯å¤šå°‘åˆ†é’Ÿï¼Ÿ", "movie_only": True},
        {"key": "scene_count_target", "question": "æ‚¨å¸Œæœ›ç”Ÿæˆå¤šå°‘åœºæˆï¼Ÿ(ç”µå½±é€šå¸¸40-100åœºï¼Œç²¾ç»†å‰§æœ¬å¯èƒ½æ›´å¤š)", "movie_only": True},
        {"key": "episode_count", "question": "æ‚¨è®¡åˆ’åˆ›ä½œå¤šå°‘é›†ï¼Ÿ", "tv_short_only": True},
        {"key": "episode_duration", "question": "æ¯ä¸€é›†çš„å¤§è‡´æ—¶é•¿æ˜¯ï¼Ÿ", "tv_short_only": True},
        
        {"key": "tone", "question": "è¿™éƒ¨ä½œå“çš„åŸºè°ƒæ˜¯ä»€ä¹ˆï¼Ÿ"},
        {"key": "time_period", "question": "æ•…äº‹å‘ç”Ÿåœ¨ä»€ä¹ˆæ—¶ä»£èƒŒæ™¯ï¼Ÿ"},
        {"key": "title", "question": "ä¸ç®¡æ˜¯æš‚å®šè¿˜æ˜¯æ­£å¼ï¼Œç»™è¿™ä¸ªæ•…äº‹èµ·ä¸ªåå­—å§ï¼Ÿ"},
        
        # Snowflake Step 2 & 4: Expansion
        {"key": "story_expansion", "question": "æˆ‘ä»¬éœ€è¦åŸºäºç›®å‰çš„æ„æ€æ‰©å±•å‡ºä¸€ä¸ªå®Œæ•´çš„ä¸‰å¹•å¼å¤§çº²ï¼Œæ‚¨æœ‰ä»€ä¹ˆç‰¹åˆ«çš„æƒ³æ³•å—ï¼Ÿ"},
        
        # Snowflake Step 3 & 5: Character focus
        {"key": "character_details", "question": "ä¸»è¦è§’è‰²çš„æ€§æ ¼ã€å¤–è²Œæˆ–èƒŒæ™¯æœ‰ä»€ä¹ˆç‰¹åˆ«è®¾å®šï¼Ÿ"},
        
        # Detailed plot
        {"key": "plot_details", "question": "æœ‰å“ªäº›ä¸€å®šè¦å‘ç”Ÿçš„å…³é”®æƒ…èŠ‚æˆ–è½¬æŠ˜ï¼Ÿ"},
        
        {"key": "theme", "question": "æ‚¨æƒ³é€šè¿‡è¿™ä¸ªæ•…äº‹æ¢è®¨ä»€ä¹ˆä¸»é¢˜ï¼Ÿ"},
        {"key": "visual_style", "question": "è§†è§‰é£æ ¼åå‘äºä»€ä¹ˆï¼Ÿ"},
        {"key": "user_notes", "question": "è¿˜æœ‰ä»€ä¹ˆè¡¥å……çš„å†…å®¹ï¼Œæˆ–è€…ç‰¹åˆ«çš„è¦æ±‚å—ï¼Ÿ"},
        
        # Final confirmation
        {"key": "final_confirm", "question": "ä»¥ä¸Šæ˜¯å‰§æœ¬çš„å®Œæ•´è®¾å®šï¼Œè¯·ç¡®è®¤æ˜¯å¦å¯ä»¥å¼€å§‹ç”Ÿæˆåˆ†åœºå¤§çº²ï¼Ÿ", "is_confirmation": True}
    ]

    # 1. Check which steps are missing
    # Important: 'project_type' is stored in column, others in global_context
    normalized_context = context.copy()
    if project.project_type and project.project_type != "pending":
        normalized_context['project_type'] = project.project_type
    
    # Calculate Total Steps (Dynamic based on Type)
    relevant_steps = []
    p_type = normalized_context.get("project_type", "movie")
    for step in REQUIRED_STEPS:
         # Filter based on type
         if step.get("movie_only") and p_type != "movie": continue
         if step.get("tv_short_only") and p_type == "movie": continue
         relevant_steps.append(step)

    next_step = None
    next_step_index = 0
    total_steps = len(relevant_steps)

    for i, step in enumerate(relevant_steps):
        if step["key"] not in normalized_context:
            next_step = step
            next_step_index = i + 1
            break
            
    # 2. If all steps completed -> Proceed to Outline Generation
    if not next_step:
        logger.info(f"é¡¹ç›® {project_id} æ‰€æœ‰åŸºç¡€è®¾å®šæ­¥éª¤å·²å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆå¤§çº²ã€‚")
        return {"type": "completed", "message": "åŸºç¡€è®¾å®šå·²å®Œæˆï¼å‡†å¤‡ç”Ÿæˆå¤§çº²..."}

    logger.info(f"é¡¹ç›® {project_id} ä¸‹ä¸€æ­¥éª¤: {next_step['key']} ({next_step_index}/{total_steps})")
    
    # helper to inject progress info
    def add_progress(payload):
        payload["progress"] = {"current": next_step_index, "total": total_steps}
        return payload

    # 3. Handle specific logic for the next step
    # 3.1 Hardcoded options for Type
    if next_step["key"] == "project_type":
        return {
            "type": "interaction_required",
            "payload": add_progress({
                "field": "project_type",
                "question": next_step["question"],
                "options": next_step["default_options"]
            })
        }
    
    # 3.2 Hardcoded options for Episode Count / Movie Duration / Scene Count
    if next_step["key"] == "movie_duration":
         return {
            "type": "interaction_required",
            "payload": add_progress({
                "field": "movie_duration",
                "question": next_step["question"],
                "options": [
                    {"label": "90åˆ†é’Ÿ (æ ‡å‡†ç”µå½±)", "value": "90"},
                    {"label": "120åˆ†é’Ÿ (é•¿ç¯‡å•†ä¸šç‰‡)", "value": "120"},
                    {"label": "150åˆ†é’Ÿä»¥ä¸Š (å²è¯—ç¯‡å¹…)", "value": "150"},
                    {"label": "60åˆ†é’Ÿ (ä¸­ç‰‡/ç”µè§†ç”µå½±)", "value": "60"}
                ]
            })
        }

    if next_step["key"] == "scene_count_target":
         return {
            "type": "interaction_required",
            "payload": add_progress({
                "field": "scene_count_target",
                "question": next_step["question"],
                "options": [
                    {"label": "40åœº (ç®€çº¦å¤§çº²)", "value": "40"},
                    {"label": "60åœº (æ ‡å‡†å¤§çº²)", "value": "60"},
                    {"label": "100åœº (ç²¾ç»†å¤§çº²)", "value": "100"},
                    {"label": "120åœºä»¥ä¸Š (æåº¦è¯¦å°½)", "value": "120"}
                ]
            })
        }

    if next_step["key"] == "episode_count":
         return {
            "type": "interaction_required",
            "payload": add_progress({
                "field": "episode_count",
                "question": next_step["question"],
                "options": [
                    {"label": "8é›† (è¿·ä½ å‰§)", "value": "8"},
                    {"label": "12é›† (æ ‡å‡†å­£)", "value": "12"},
                    {"label": "20é›† (å›½äº§å‰§æ ‡å‡†)", "value": "20"},
                    {"label": "24é›†", "value": "24"},
                    {"label": "40é›†ä»¥ä¸Š", "value": "40"}
                ]
            })
        }
    
    if next_step["key"] == "episode_duration":
         return {
            "type": "interaction_required",
            "payload": add_progress({
                "field": "episode_duration",
                "question": next_step["question"],
                "options": [
                    {"label": "1-2åˆ†é’Ÿ (ç«–å±çŸ­å‰§)", "value": "2mins"},
                    {"label": "5-10åˆ†é’Ÿ (è¿·ä½ å‰§)", "value": "10mins"},
                    {"label": "20åˆ†é’Ÿ (æƒ…æ™¯å–œå‰§/åŠ¨ç”»)", "value": "20mins"},
                    {"label": "45åˆ†é’Ÿ (æ ‡å‡†å‰§é›†)", "value": "45mins"},
                    {"label": "60åˆ†é’Ÿ (ç¾å‰§/ç”µå½±æ„Ÿ)", "value": "60mins"}
                ]
            })
        }
    
    if next_step.get("is_confirmation"):
        # Format a summary for confirmation
        summary_lines = []
        for k, v in normalized_context.items():
             summary_lines.append(f"- {k}: {v}")
        summary_text = "\n".join(summary_lines)
        return {
            "type": "interaction_required",
            "payload": add_progress({
                "field": "final_confirm",
                "question": next_step["question"],
                "context_summary": summary_text,
                "options": [
                    {"label": "âœ… ç¡®å®šå¹¶å¼€å§‹ç”Ÿæˆ", "value": "confirmed"},
                    {"label": "ğŸ”„ æˆ‘æƒ³ä¿®æ”¹ä¸€äº›å†…å®¹", "value": "reset"}
                ]
            })
        }
    
    # 3.4 Check Prompt Richness (Optimization)
    # If the user's initial logline is very long (> 100 chars) and detailed,
    # we tell the LLM to verify if we even need to ask this question.
    # Note: Currently we just proceed to ask to be comprehensive.
    
    # 3.4 For other steps, use LLM to generate context-aware options
    # We pass the logline + current context to LLM
    prompt_context = f"Logline: {project.logline}\nCurrent Settings: {json.dumps(normalized_context, ensure_ascii=False)}"
    
    logger.info(f"æ­£åœ¨è°ƒç”¨ LLM ä¸ºæ­¥éª¤ {next_step['key']} ç”Ÿæˆé€‰é¡¹...")
    
    # 3.2 For other steps, use LLM to generate context-aware options
    try:
        question_data, usage = await llm.generate_interaction_options(
            step_key=next_step["key"],
            base_question=next_step["question"],
            context_str=prompt_context
        )
        # Log AI action
        background_tasks.add_task(
            log_ai_action,
            user_id=current_user.id,
            project_id=project_id,
            action=f"analyze_step_{next_step['key']}",
            prompt=prompt_context,
            response=str(question_data),
            tokens=usage
        )
    except Exception as e:
        logger.error(f"LLM äº¤äº’ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(
            status_code=503, 
            detail=f"AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ API Key é…ç½®æˆ–ç¨åé‡è¯• ({str(e)})"
        )
    
    # Update Token Usage
    project.total_tokens += usage
    
    # Construction Response
    response_payload = {
        "type": "interaction_required",
        "payload": add_progress({
            "field": next_step["key"],
            "question": question_data.get("question", next_step["question"]), 
            "options": question_data.get("options", [])
        })
    }
    
    # Cache the result to DB so next fetch is instant
    project.next_step_cache = response_payload
    await db.commit()

    return response_payload

@app.post("/projects/{project_id}/generate_scenes")
async def generate_scenes(
    project_id: int, 
    selected_option: str = None, 
    background_tasks: BackgroundTasks = BackgroundTasks(),
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    """
    Phase 1.5: User selected an option, now generate outline.
    Phase 2: Add background task for generation.
    """
    logger.info(f"æ”¶åˆ°ç”Ÿæˆåˆ†åœºå¤§çº²è¯·æ±‚ï¼Œé¡¹ç›®ID: {project_id}")
    # 1. Update project genre/style based on selected_option
    project = await db.get(models.Project, project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")
    
    # Use selected_option if string generic, or fallback to stored context values
    style_context = selected_option
    if not style_context:
        # Construct summary from context
        c = project.global_context or {}
        style_context = f"Genre: {project.project_type}, Tone: {c.get('tone')}, Style: {c.get('visual_style')}"

    # Extract target episode count / scene count from context
    c = project.global_context or {}
    target_count = 5
    
    # Priority for Movie: scene_count_target
    if project.project_type == "movie":
        raw_count = c.get("scene_count_target")
    else:
        raw_count = c.get("episode_count")

    if raw_count:
        try:
            if isinstance(raw_count, int):
                target_count = raw_count
            elif isinstance(raw_count, str):
                import re
                # Try to find first number
                digits = re.findall(r'\d+', raw_count)
                if digits:
                    target_count = int(digits[0])
        except Exception as e:
            logger.warning(f"Error parsing count: {e}")
            
    # If movie duration is set but scene count isn't, estimate
    if project.project_type == "movie" and not c.get("scene_count_target"):
        duration = c.get("movie_duration")
        if duration:
            try:
                # 1.5 scenes per minute is a high-detail script, 0.5 is low. 1.0 is standard.
                target_count = int(int(re.findall(r'\d+', str(duration))[0]) * 0.8)
            except: pass

    project.genre = style_context
    project.status = models.ProcessingStatus.GENERATING
    # Force clearing of any old scenes from a previous attempt
    await db.execute(delete(models.Scene).where(models.Scene.project_id == project_id))
    await db.commit()

    logger.info(f"å¯åŠ¨åå°ä»»åŠ¡ç”Ÿæˆå¤§çº²... (Style: {style_context}, Count: {target_count})")
    
    # 2. Trigger Background Task for Incremental Outline Generation
    background_tasks.add_task(
        run_incremental_outline_generation, 
        project_id, 
        style_context, 
        target_count,
        current_user.id
    )
    
    return {"status": "Scene generation started", "project_id": project_id}

# --- Background Task Implementation ---
from sqlalchemy import delete

async def run_incremental_outline_generation(project_id: int, style_context: str, target_count: int, user_id: int):
    logger.info(f"[Task] Starting Incremental Outline Gen for Project {project_id}")
    
    async with database.SessionLocal() as db:
        project = await db.get(models.Project, project_id)
        if not project: return
        
        # Determine Batch Size (User requested "safe/one-by-one", so we choose 1 to be absolutely safe and responsive)
        # Using 1 allows frontend to see each scene pop up.
        batch_size = 1 
        current_idx = 1
        last_context = "Start of story."
        
        while current_idx <= target_count:
            # Re-check status in case user cancelled
            await db.refresh(project)
            if project.status == models.ProcessingStatus.FAILED:
                logger.info("[Task] Outline Gen Cancelled.")
                return 

            end_idx = min(current_idx + batch_size - 1, target_count)
            logger.info(f"[Task] Generating scenes {current_idx}-{end_idx}...")
            
            try:
                batch_scenes, usage = await llm.generate_scene_batch(
                    project.logline, 
                    style_context, 
                    current_idx, 
                    end_idx, 
                    previous_context=last_context,
                    total_target=target_count
                )
                
                project.total_tokens += usage
                
                # If success, save to DB immediately
                if batch_scenes:
                    for s_data in batch_scenes:
                        new_scene = models.Scene(
                            project_id=project.id,
                            scene_index=s_data.get("index", current_idx),
                            outline=s_data.get("outline", "Unknown"),
                            status=models.ProcessingStatus.PENDING
                        )
                        db.add(new_scene)
                    
                    # Update context for next batch
                    summaries = [s.get('outline', '') for s in batch_scenes]
                    last_context = "; ".join(summaries) # Keep it short
                else:
                    # Fallback for empty/failure
                    logger.error(f"[Task] Batch {current_idx} failed.")
                    new_scene = models.Scene(
                        project_id=project.id,
                        scene_index=current_idx,
                        outline="[ç”Ÿæˆå¤±è´¥] è¯·ç¨åå°è¯•é‡å†™æ­¤åœºã€‚",
                        status=models.ProcessingStatus.PENDING
                    )
                    db.add(new_scene)

                await db.commit()
                
            except Exception as e:
                logger.error(f"[Task] Critical error in outline batch: {e}")
            
            current_idx += batch_size
        
        # After Outline Complete -> Trigger Content Generation
        logger.info("[Task] Outline Complete. Starting Content Gen Loop...")
        # Since we are not in a request scope, we can't use BackgroundTasks object easily to chain.
        # But we can just await the next function directly since we are already in an async background loop.
        await run_generation_loop(project.id)


@app.post("/projects/{project_id}/scenes/{scene_index}/regenerate")
async def regenerate_scene(
    project_id: int, 
    scene_index: int,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    project = await db.get(models.Project, project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")
        
    result = await db.execute(
        select(models.Scene)
        .where(models.Scene.project_id == project_id)
        .where(models.Scene.scene_index == scene_index)
    )
    scene = result.scalars().first()
    if not scene:
        raise HTTPException(status_code=404, detail="Scene not found")
        
    # Reset status
    scene.status = models.ProcessingStatus.PENDING
    scene.content = None # Clear old content
    if project.status == models.ProcessingStatus.COMPLETED:
        project.status = models.ProcessingStatus.GENERATING
        
    await db.commit()
    
    # Trigger loop again
    background_tasks.add_task(run_generation_loop, project.id)
    return {"status": "Regeneration scheduled"}

# --- Export (New) ---
import io
# Try imports, fallback to plain text if failed
try:
    from docx import Document as DocxDocument
except ImportError:
    DocxDocument = None
try:
    from reportlab.pdfgen import canvas
    from reportlab.lib.pagesizes import A4
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
except ImportError:
    canvas = None

from fastapi.responses import StreamingResponse

@app.get("/projects/{project_id}/export")
async def export_project(
    project_id: int, 
    format: str = "txt",
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    # Eager load scenes
    result = await db.execute(
        select(models.Project)
        .where(models.Project.id == project_id)
        .options(selectinload(models.Project.scenes))
    )
    project = result.scalars().first()
    
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")

    filename_raw = project.title or 'Untitled_Script'
    filename_encoded = quote(filename_raw)
    
    # Prepare Content Data
    project_scenes = sorted(project.scenes, key=lambda s: s.scene_index)
    context = project.global_context or {}
    
    if format == "docx":
        if not DocxDocument:
            raise HTTPException(501, "Word export library (python-docx) not installed on server.")
        
        doc = DocxDocument()
        doc.add_heading(project.title or "Untitled", 0)
        
        doc.add_heading("Project Bible", level=1)
        doc.add_paragraph(f"Logline: {project.logline}")
        doc.add_paragraph(f"Type: {project.project_type} | Genre: {project.genre}")
        for k, v in context.items():
            if k not in ['logline', 'project_type']:
                try:
                     doc.add_paragraph(f"{str(k).capitalize()}: {str(v)}")
                except:
                     pass
                
        doc.add_page_break()
        doc.add_heading("Screenplay", level=1)
        
        for scene in project_scenes:
            doc.add_heading(f"SCENE {scene.scene_index}", level=2)
            doc.add_paragraph(f"Outline: {scene.outline}", style='Intense Quote')
            if scene.content:
                # Basic formatting for script
                doc.add_paragraph(scene.content)
            else:
                doc.add_paragraph("[Content Generating...]")
            doc.add_paragraph("") # Spacing
            
        buffer = io.BytesIO()
        doc.save(buffer)
        buffer.seek(0)
        
        return StreamingResponse(
            buffer, 
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            headers={"Content-Disposition": f"attachment; filename*=UTF-8''{filename_encoded}.docx"}
        )

    elif format == "md":
        content = f"# {project.title or 'Untitled'}\n\n"
        content += f"**Logline:** {project.logline}\n\n"
        content += f"**Type:** {project.project_type}\n"
        content += "---\n\n## Project Settings\n"
        for k, v in context.items():
             content += f"- **{k}:** {v}\n"
        content += "\n---\n\n## Script\n\n"
        
        for scene in project_scenes:
            content += f"### SCENE {scene.scene_index}\n"
            content += f"> **Outline:** {scene.outline}\n\n"
            content += (scene.content or "[Generating...]") + "\n\n"
            content += "---\n\n"
            
        return StreamingResponse(
            io.BytesIO(content.encode('utf-8')),
            media_type="text/markdown",
            headers={"Content-Disposition": f"attachment; filename*=UTF-8''{filename_encoded}.md"}
        )
        
    else: # Default TXT
        content = f"Title: {project.title}\nLogline: {project.logline}\n\n"
        for scene in project_scenes:
            content += f"SCENE {scene.scene_index}\n{scene.content or ''}\n\n"
        return StreamingResponse(
            io.BytesIO(content.encode('utf-8')),
            media_type="text/plain",
            headers={"Content-Disposition": f"attachment; filename*=UTF-8''{filename_encoded}.txt"}
        )

# --- Background Task (The Engine) ---

async def run_generation_loop(project_id: int):
    """
    The Core Loop: Iterates scenes and generates content with Rolling Summary.
    """
    logger.info(f"[åå°ä»»åŠ¡] å¼€å§‹ä¸ºé¡¹ç›® {project_id} ç”Ÿæˆå‰§æœ¬å†…å®¹...")
    
    # Create a new session for the background task
    async with database.SessionLocal() as db:
        # Load Project Info
        project = await db.get(models.Project, project_id)
        if not project: 
            logger.error(f"[åå°ä»»åŠ¡] é¡¹ç›® {project_id} æœªæ‰¾åˆ°ï¼Œä»»åŠ¡ä¸­æ­¢")
            return

        # Load scenes
        result = await db.execute(
            select(models.Scene)
            .where(models.Scene.project_id == project_id)
            .order_by(models.Scene.scene_index)
        )
        scenes = result.scalars().all()
        
        cumulative_context = ""

        for scene in scenes:
            # Re-Check Status (User might have deleted/paused)
            await db.refresh(project)
            if project.status == models.ProcessingStatus.FAILED: # Treat as stop signal
                logger.info("[åå°ä»»åŠ¡] æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œä»»åŠ¡ä¸­æ­¢")
                break

            if scene.status == models.ProcessingStatus.COMPLETED:
                continue # Skip already generated

            # 1. Mark as Generating
            logger.info(f"[åå°ä»»åŠ¡] æ­£åœ¨ç”Ÿæˆç¬¬ {scene.scene_index} åœº: {scene.outline[:30]}...")
            scene.status = models.ProcessingStatus.GENERATING
            await db.commit()
            
            # 2. Call LLM to Write Scene
            generated_content, usage = await llm.write_scene_content(
                logline=project.logline,
                style_guide=project.genre,
                current_scene_outline=scene.outline,
                previous_context=cumulative_context
            )
            
            project.total_tokens += usage

            # Log AI Action (Direct call since we are already in background)
            await log_ai_action(
                user_id=project.owner_id,
                project_id=project.id,
                action=f"write_scene_{scene.scene_index}",
                prompt=f"Outline: {scene.outline}, PrevContextLength: {len(cumulative_context)}",
                response=generated_content if generated_content else "Error/Empty",
                tokens=usage
            )

            # 3. Update Content
            if generated_content:
                scene.content = generated_content
                # Simple rolling context for now (first 200 chars to avoid token limit in basic version)
                cumulative_context += f"\n[Scene {scene.scene_index} Summary]: {scene.outline}" 
                logger.info(f"[åå°ä»»åŠ¡] ç¬¬ {scene.scene_index} åœºç”Ÿæˆå®Œæˆ")
            else:
                scene.content = "(AI Generation Failed)"
                logger.error(f"[åå°ä»»åŠ¡] ç¬¬ {scene.scene_index} åœºç”Ÿæˆå†…å®¹ä¸ºç©º")

            scene.status = models.ProcessingStatus.COMPLETED
            await db.commit()
        
        # Mark Project Complete
        project.status = models.ProcessingStatus.COMPLETED
        await db.commit()
        logger.info(f"[åå°ä»»åŠ¡] é¡¹ç›® {project_id} æ‰€æœ‰å‰§æœ¬ç”Ÿæˆä»»åŠ¡å®Œæˆï¼")
            
    print(f"Generation loop finished for Project {project_id}")

import database # Import at end to avoid circular dependency issues in loop if needed
